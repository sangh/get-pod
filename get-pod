#!/usr/bin/env python3

"""
Usage: get-pod

See the example conf file for a description of how to use this script.
This script reads the conf file and downloads podcasts.  It can ask which
ones to get or do so automatically.

If "-a" or "--ask" is given each show is treated as if the "ask" mode
was specified and the tag to use was specified as "ask" (which asks for the
field to use).

The default conf file is "~/.get-pod.conf", to use another file pass
an "--" argument followed by the file name to use or just the file name if
that file name doesn't start with the "-" character.

This program uses threads and assumes that this script has *exclusive*
access to both the conf file (for reading) and the status and work directories
specified in it for both reading and writing.  Each thread only touches a
single file so they shouldn't need coordination among themselves.
"""

import urllib.request
import os, sys, time, json, queue, tempfile, threading, subprocess
import feedparser


def wrn(msg):
    print(msg, file=sys.stderr)

def bye(msg):
    wrn(msg)
    sys.exit(1)


arg_ask = False
def args_set(a):
    global arg_ask
    if a in ("-a", "--ask"):
        arg_ask = True
    else:
        bye("Unknown argument: %s" % arg)

conf_filename = None
seen_flag_stop = False
for arg in sys.argv[1:]:
    if seen_flag_stop:
        if conf_filename:
            bye("Second conf file name given: %s" % fn)
        conf_filename = arg
    elif arg == "--":
        seen_flag_stop = True
    elif arg[0] == "-":
        args_set(arg)
    else:
        if conf_filename:
            bye("Second conf file name given: %s" % fn)
        conf_filename = arg

if not conf_filename:
    conf_filename = os.path.join(os.path.expanduser("~"), ".get-pod.conf")

try:
    with open(conf_filename, "r", encoding="UTF-8") as f:
        exec(f.read())
except:
    bye("Cannot access conf file: %s" % conf_filename)

# Won't print anything, but will error if not defined or castable to an int.
int(max_connections)


# Allow conf file to override, and distingush between defined and non-float.
try:
    seconds_between_printing_thread_state
except:
    seconds_between_printing_thread_state = 120.0
float(seconds_between_printing_thread_state)  # Error if not a float.


# These need to be str, see the comment called *Unicode Filenames* below.
for d in (status_dir, work_dir, putjoysuck):
    if not isinstance(d, str):
        bye("Must of of type str (see script comments): %s" % str(d))

if not os.path.isdir(status_dir) or \
        not os.access(status_dir, os.F_OK | os.R_OK | os.X_OK | os.W_OK):
    bye("The status directory will not be created automatically, so if this\n"
        "is your first time running this program you may want to run:\n"
        "    mkdir '%s'\nand try again.\n"
        "Cannot access status directory: %s" % (status_dirm, status_dir))

if not os.path.exists(work_dir):
    wrn("Creating work directory: %s" % work_dir)
    os.mkdir(work_dir)
if not os.path.isdir(work_dir) or \
        not os.access(work_dir, os.F_OK | os.R_OK | os.X_OK | os.W_OK):
    bye("Cannot access work directory: %s" % work_dir)
if not len(os.listdir(work_dir)) == 0:
    bye("Work dir is not empty: %s" % work_dir)

if not os.access(putjoysuck, os.F_OK | os.R_OK | os.X_OK):
    bye("Seems that \"%s\" isn't a file/readable/executable." % putjoysuck)

# The conf file is a tuple of tuples, we go through it, transforming it to a
# dict with the short name being the key checking for duplicates, setting the
# ignore bozo function if not set, and changing the mode and tag_field to
# ask if the ask argument was passed.  Error if anything is amiss.
feeds_conf = feeds
feeds = {}
for f in feeds_conf:
    if not len(f) in (4, 5):
        bye("Feed spec is wrong length: %s" % str(f))

    # All these need to be str, not bytes because short-name is used as
    # a file name (see the comment called *Unicode Filenames* below), and the
    # others are going to be JSON serialized, so seralizing and then
    # deserialized and compared with the str taken from the etag or modified
    # HTTP headers, which will never work with bytes.
    if not isinstance(f[0], str):
        bye("Short-names must be of type str: %s" % str(f[0]))
    if not isinstance(f[1], str):
        bye("Mode must be of type str: %s" % str(f[1]))
    if not isinstance(f[2], str):
        bye("Tag-field must be of type str: %s" % str(f[2]))
    if not isinstance(f[3], str):
        bye("URL must be of type str: %s" % str(f[3]))

    sn = f[0]
    if sn in feeds:
        bye("Duplicate short-name in feeds: %s" % sn)

    feeds[sn] = {}

    if arg_ask:
        feeds[sn]["mode"] = "ask"
        feeds[sn]["tag_field"] = "ask"
    else:
        if not f[1] in ("get", "select", "all", "ask", "ignore"):
            bye("Unknown mode for %s: %s." % (sn, f[1]))
        feeds[sn]["mode"] = f[1]
        feeds[sn]["tag_field"] = f[2]

    feeds[sn]["url"] = f[3]

    try:
        feeds[sn]["ignore_bozo_function"] = f[4]
    except:
        feeds[sn]["ignore_bozo_function"] = lambda bozo_exp_str: False

    if not callable(feeds[sn]["ignore_bozo_function"]):
        bye("Not callable: %s" % str(feeds[sn]["ignore_bozo_function"]))

    feeds[sn]["etag"] = None
    feeds[sn]["modified"] = None
    feeds[sn]["downloaded_links"] = []
    feeds[sn]["new_downloaded_links"] = set()

# *Unicode Filenames*
# In Linux/Unix (NOT macOS) the bytes representation of a file name is all
# that is recorded in the file system.  In Windows the file name encoding
# changes based on the current default code-page and is sometimes
# case-insensitive.  In macOS they almost use UTF-8, but will decompose all
# characters (use combining characters whenever possible).  This sucks.  You
# would hope that there would be some way in Python to create a file and make
# sure when you call os.listdir you get a str/bytes that compares to True
# with the original str/bytes you used to create the file, but you can't.
# At all.  On Linux/Unix you can use bytes and this will work.  On Windows
# there is a PEP to make using str (NOT bytes) always to do right thing based
# on the default code-page so that that will work, though if you use a case-
# insensitive file system that won't always work.  And on macOS you can't do
# it at all because macOS will save the file decomposed but still allow you
# to open it with the composed name.  Which means when you run os.listdir
# you get back a different encoding.  So, for example, if you create a file
# called b'_\xc3\xa9_' you can open it with all the following arguments:
#    b'_\xc3\xa9_'
#    b'_e\xcc\x81_'
#    '_é_'
# All of which refer to the same file.  And when you call os.listdir you
# always get b'_e\xcc\x81_' or '_é_' (with the decomposed code-points)
# even when you used b'_\xc3\xa9_' or '_é_' (with a composed code-point) to
# create it.
#
# Anyway.  What this means for this script is that I will always use str to
# store file paths (never bytes) and then when a status file for a given
# short-name is created will run os.listdir and see if it compares as equal.
# If not an error will be printed saying to change the short-name in the
# conf file to something that the file system won't munge.
existing_status_files = os.listdir(status_dir)
for sn in feeds:
    try:
        # If we can remove, then the file is there.
        existing_status_files.remove(sn)
    except:
        wrn("Creating status file for \"%s\"." % sn)
        # We create a directory inside the status dir (to make sure it's on
        # the same filesystem) and try and create a file with this name, and
        # then call listdir to read it back to make sure the file system
        # isn't munging it.  See the comment *Unicode Filenames* above.
        tmp_dir = os.path.join(status_dir, 'tmp')
        try:
            os.mkdir(tmp_dir)
        except:
            bye("Could not create tmp dir inside the status dir."
                "Need to do this to make sure it's empty before running the"
                "listdir on it.  Please remove this directory before running"
                "this again: %s" % tmp_dir)
        try:
            with open(os.path.join(tmp_dir, sn), "w", encoding="UTF-8") as f:
                f.write('Test file, please delete.\n')
        except:
            pass
        fs = os.listdir(tmp_dir)
        for f in fs:
            os.remove(os.path.join(tmp_dir, f))
        os.rmdir(tmp_dir)
        if not sn in fs:
            bye("The short-name \"%s\" is being munged by the file system:\n"
                "%s\nEven if there are other fixes (like normalization on "
                "macOS, using str on \nWindows, and using bytes on "
                "Linux/Unix) they are annoying, might not always \nwork, and "
                "might change unexpectedly in the future, so instead you "
                "should \njust change the short name you are using to "
                "something that won't be munged by \nthe file system you are "
                "using." % (
                    str(bytes(sn, "UTF-8")),
                    str([str(bytes(s, "UTF-8")) for s in fs]),
                )
            )
        with open(os.path.join(status_dir, sn), "w", encoding="UTF-8") as f:
            json.dump(
                {"etag": None, "modified": None, "downloaded_links": []},
                f)

    # No matter if it's ignored or asked, etc., we try and read the status.
    with open(os.path.join(status_dir, sn), "r", encoding="UTF-8") as f:
        st = json.load(f)
    feeds[sn]["etag"] = st["etag"]
    feeds[sn]["modified"] = st["modified"]
    feeds[sn]["downloaded_links"] = st["downloaded_links"]

if len(existing_status_files) != 0:
    bye("Unknown status file(s) found:\n%s\n"
        "Either remove the status file(s) or create entries in the conf file"
        " (with\n a mode of \"ignore\" if you don't want the status file to"
        " be read at all)." % str(existing_status_files))


# Save result of what we do with each feed.
feeds_stats_ignored = []  # List of short-names.
feeds_stats_unchanged = []  # List of 3-tuples of (sn, etag, modified).
feeds_stats_errors = []  # List of 2-tuples of (short-name, error-message).
feeds_stats_downloads = {}  # sn -> [n_down, n_skipped, [errors, ...]]

feeds_to_fetch = []  # List of short-names.
feeds_to_fetch_ignore_etag_modified = []  # List of short-names.
feeds_to_ask_if_fetch = []  # List of short-names.
for sn in feeds:
    if "ignore" == feeds[sn]["mode"]:
        feeds_stats_ignored.append(sn)
    elif feeds[sn]["mode"] in ("get", "select"):
        feeds_to_fetch.append(sn)
    elif "all" == feeds[sn]["mode"]:
        feeds_to_fetch_ignore_etag_modified.append(sn)
    elif "ask" == feeds[sn]["mode"]:
        feeds_to_ask_if_fetch.append(sn)
    else:
        bye("Unknown mode \"%s\" for \"%s\"." % (str(feeds[sn]["mode"]), sn))


# Now that all the errors checking is done, there are two places that do
# I/O, fetching the feeds, and downloading each episode of them. Those are
# done in threads, and everything else is done in this main thread.

# Two thread pools are used one to fetch and parse the XML with feedparser,
# and the second to download episodes.  These two pools must both check the
# same semaphone to make sure max_connections isn't exceeded (and since they
# are both just doing I/O there doesn't need to be more than that many
# threads in each).
max_connections_semaphore = threading.BoundedSemaphore(max_connections)
# Assume given program is NOT thread-safe.
putjoysuck_semaphore = threading.BoundedSemaphore(1)
# We send all the feeds to fetch either respecting etags/modified-times or
# not, while asking about the rest of feeds, then take the results (after
# asking about all the feeds) and asking (if needed) about each episode.
# For all the threads if they get a None on the queue they should exit.
# 4-tuple with (sn, url, etag, modified)
fetch_in_queue = queue.Queue()
# 2-tuple with (sn, parsed_feed_from_feedparser)
fetch_out_queue = queue.Queue()

def fetch_thread_function(local):
    global max_connections_semaphore
    global fetch_in_queue
    global fetch_out_queue
    global feeds_stats_errors
    # Everything else should be local.
    while True:
        local.item = fetch_in_queue.get()
        if not local.item:
            return None

        # We should have gotten a 4-tuple with (sn, url, etag, modified).
        local.sn = local.item[0]
        local.url = local.item[1]
        local.etag = local.item[2]
        local.modified = local.item[3]

        try:
            max_connections_semaphore.acquire()

            if local.etag and local.modified:
                local.feed = feedparser.parse(local.url,
                                            etag=local.etag,
                                        modified=local.modified)
            elif local.etag:
                local.feed = feedparser.parse(local.url,
                                            etag=local.etag)
            elif local.modified:
                local.feed = feedparser.parse(local.url,
                                            modified=local.modified)
            else:
                local.feed = feedparser.parse(local.url)
        except:
            feeds_stats_errors.append((local.sn, str(sys.exc_info()[1])))
            continue
        finally:
            max_connections_semaphore.release()

        fetch_out_queue.put((local.sn, local.feed))


# (sn, [url, ...], desc, pub_dt)
download_in_queue = queue.Queue()
# (sn, [url, ...], str(error/success message))
download_out_queue = queue.Queue()

def download_thread_function(local):
    """Should download and call putjoysuck, returning the error string
    and url's to record."""
    global max_connections_semaphore
    global putjoysuck_semaphore
    global download_in_queue
    global download_out_queue
    global putjoysuck
    global work_dir
    # Everything else should be local.
    while True:
        local.item = download_in_queue.get()
        if not local.item:
            return None

        local.sn = local.item[0]
        local.urls = local.item[1]
        local.desc = local.item[2]
        local.pub_dt = local.item[3]

        local.f, local.fn = tempfile.mkstemp(   suffix='',
                                                prefix='%s-' % local.sn,
                                                dir=work_dir,
                                                text=False)
        os.close(local.f)
        # Define here so we can include in error messages.
        local.headers = None
        local.putjoysuck_cmd = [putjoysuck,
                local.sn, local.desc, local.fn, local.pub_dt]

        try:
            max_connections_semaphore.acquire()
            local.fn, local.headers = urllib.request.urlretrieve(
                    local.urls[0], filename=local.fn)
        except:
            msg = "Error getting %s:\n%s\n%s\n%s\nWould have run: %s" % (
                local.sn,
                str(local.urls),
                str(local.headers),
                str(sys.exc_info()[1]),
                str(local.putjoysuck_cmd),
            )
            download_out_queue.put((local.sn, [], msg))
            continue
        finally:
            max_connections_semaphore.release()

        try:
            putjoysuck_semaphore.acquire()
            p = subprocess.Popen(   local.putjoysuck_cmd,
                                    stdin=subprocess.DEVNULL,
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            o, e = p.communicate()
            if 0 != p.returncode or o or e:
                download_out_queue.put((
                    local.sn,
                    [],
                        "Error running:\n"
                        "%s\n"
                        "return code = %d\n"
                        "out:\n%s\n"
                        "err:\n%s" % (
                            str(local.putjoysuck_cmd),
                            p.returncode,
                            str(o),
                            str(e),
                        )
                ))
                continue
        except:
            download_out_queue.put((
                local.sn,
                [],
                "Error running:\n%s\n%s" % (
                    str(local.putjoysuck_cmd),
                    str(sys.exc_info()[1]),
                )
            ))
            continue
        finally:
            putjoysuck_semaphore.release()

        os.remove(local.fn)
        download_out_queue.put((local.sn, local.urls, "%s: got %s %s" % (
            local.sn, local.pub_dt, local.desc)))


def shuttle_fetch_out_to_download_in():
    global feeds
    global fetch_out_queue
    global download_in_queue
    global feeds_stats_downloads

    shows_to_select_from = []
    shows_to_get_from = []

    while True:
        try:
            sn, feed = fetch_out_queue.get_nowait()
        except:
            break

        if "etag" in feed:
            feeds[sn]["etag"] = feed.etag
        else:
            feeds[sn]["etag"] = None

        if "modified" in feed:
            feeds[sn]["modified"] = feed.modified
        else:
            feeds[sn]["modified"] = None

        if "status" in feed and feed.status == 304:
            feeds_stats_unchanged.append((
                sn, feeds[sn]["etag"], feeds[sn]["modified"]))
            continue

        if feed.bozo > 0:
            if not feeds[sn]["ignore_bozo_function"](str(feed.bozo_exception)):
                feeds_stats_errors.append((sn, str(feed.bozo_exception)))
                continue

        if feeds[sn]["mode"] == "get":
            shows_to_get_from.append((sn, feed))
        elif feeds[sn]["mode"] in ("all", "ask", "select"):
            shows_to_select_from.append((sn, feed))
        else:
            wrn("Bad mode \"%s\" post fetch in %s." % (feeds[sn]["mode"], sn))

    # We always send the plain gets first, and then ask, expecting we'll
    # be called repeatedly until all the feeds are shuttled.
    # But we only really need one loop through them.
    shows = []  # 3-tuple, (select (True/False), sn, feed)
    for s in shows_to_get_from:
        shows.append((False, s[0], s[1]))
    for s in shows_to_select_from:
        shows.append((True, s[0], s[1]))
    for show in shows:
        select = show[0]
        sn = show[1]
        feed = show[2]
        n_entry_curr = 0
        n_entries_in_feed = len(feed.entries)
        if 0 == n_entries_in_feed:
            wrn("No entries in %s, which should not happen." % (sn, ))
        # So every now and again a feed just does not have _any_ dates
        # included, so updated_parsed is the only date available.
        feed.entries.reverse()  # Oldest first.
        for entry in sorted(feed.entries, key=lambda x:x.updated_parsed):
            n_entry_curr = n_entry_curr + 1
            if "published_parsed" in entry:
                pub_dt = time.strftime("%a%d%b%y-%H%M", entry.published_parsed)
            else:
                pub_dt = time.strftime("%a%d%b%y-%H%M", time.localtime())

            if "title" == feeds[sn]["tag_field"] and "title" in entry:
                tag = entry.title
            elif "published" == feeds[sn]["tag_field"]:
                if "published_parsed" in entry:
                    tag = time.strftime("%a%d%b%Y", entry.published_parsed)
                elif "updated_parsed" in entry:
                    tag = time.strftime("%a%d%b%Y", entry.updated_parsed)
                else:
                    tag = time.strftime("%a%d%b%Y", time.localtime())
            elif "date" == feeds[sn]["tag_field"]:
                tag = time.strftime("%a%d%b%Y", time.localtime())
            elif "ask" == feeds[sn]["tag_field"]:
                wrn("")
                wrn("%s: %d / %d" % (sn, n_entry_curr, n_entries_in_feed))
                for t in ("title", "summary", "content", "published",
                        "updated", "created", "expired", "id", "author",
                        "contributors", "publisher", "comments", "licence"):
                    if t in entry:
                        wrn("%s: %s" % (t, entry[t]))
                tag = input("What do you want to use as the tag? ")
                if len(tag) == 0:
                    wrn("Tag cannot be blank, using current time.")
                    tag = time.strftime("%a%d%b%Y", time.localtime())
            else:
                wrn("Unknown tag_field here (using date): %s" % (
                    feeds[sn]["tag_field"],
                ))
                tag = time.strftime("%a%d%b%Y", time.localtime())


            # Annoyingly entry.id might be the only url, and if not we treat
            # it as one because it is supposed to be a unique ID, so if a
            # feed changes all its download URLs, this will still match.
            if "id" in entry:
                guid = entry.id
            else:
                guid = None

            if not "enclosures" in entry or not entry.enclosures:
                wrn("%s has an entry with no enclosures, skipping." % sn)
                continue
            # Also, there could be multiple enclosures (links so we need to
            # loop through them all (if guid matches skip all of them).
            for enc in entry.enclosures:
                if enc.href in feeds[sn]["downloaded_links"] or \
                        (guid and guid in feeds[sn]["downloaded_links"]):
                    feeds[sn]["new_downloaded_links"].add(enc.href)
                    if guid:
                        feeds[sn]["new_downloaded_links"].add(guid)
                    if not sn in feeds_stats_downloads:
                        feeds_stats_downloads[sn] = [0, 0, []]
                    feeds_stats_downloads[sn][1] = \
                            feeds_stats_downloads[sn][1] + 1
                    continue

                if select:
                    wrn("")
                    wrn("%s: %d / %d" % (sn, n_entry_curr, n_entries_in_feed))
                    for t in ("title", "summary", "content", "published",
                            "updated", "created", "expired", "id", "author",
                            "contributors", "publisher", "comments", "licence"):
                        if t in entry:
                            wrn("%s: %s" % (t, entry[t]))
                    wrn(enc.href)
                    act = input("Do you want to {s}kip, {m}ark (and not "
                        "downloaded), or {d}ownload (and mark)? [skip] ")
                    if len(act) < 1:
                        act = "s"
                    if act[0] in ("m", "M"):
                        wrn("Marking.")
                        feeds[sn]["new_downloaded_links"].add(enc.href)
                        if guid:
                            feeds[sn]["new_downloaded_links"].add(guid)
                        continue
                    elif act[0] in ("d", "D"):
                        wrn("Queueing download.")
                    else:
                        wrn("Skipping")
                        continue

                # sn, urls..., desc, pub_dt
                urls = [enc.href, ]
                if guid:
                    urls.append(guid)
                download_in_queue.put((sn, urls, tag, pub_dt))
                continue




t_local = threading.local()
for i in range(max_connections):
    threading.Thread(target=fetch_thread_function, args=(t_local, )).start()
    threading.Thread(target=download_thread_function, args=(t_local, )).start()

if threading.active_count() != 2 * max_connections + 1:
    bye("Not all threads started.  Active count = %d, max_conn = %d" % (
        threading.active_count(), max_connections))
# When active count is this much all the fetch threads have terminated.
t_active_count_after_fetch = max_connections + 1

for sn in feeds_to_fetch:
    fetch_in_queue.put((
        sn, feeds[sn]["url"], feeds[sn]["etag"], feeds[sn]["modified"]))
for sn in feeds_to_fetch_ignore_etag_modified:
    fetch_in_queue.put((sn, feeds[sn]["url"], None, None))

for sn in feeds_to_ask_if_fetch:
    act = input("Fetch XML for %s? [no] " % sn)
    if len(act) > 0 and act[0] in ("y", "Y"):

        act = input("Check if entries have already been downloaded? [yes] ")
        if len(act) > 0 and act[0] in ("n", "N"):
            wrn("Clearing downloaded links.")
            feeds[sn]["downloaded_links"] = []
        else:
            wrn("Will use downloaded links.")

        act = input("Respect etag/modified-time? [yes] ")
        if len(act) > 0 and act[0] in ("n", "N"):
            wrn("Queueing fetch without respecting etags/modified-time.")
            fetch_in_queue.put((sn, feeds[sn]["url"], None, None))
        else:
            wrn("Queueing fetch (using etags/modified-time).")
            fetch_in_queue.put((sn, feeds[sn]["url"],
                feeds[sn]["etag"], feeds[sn]["modified"]))
    else:
        wrn("Skipping %s" % sn)
        feeds_stats_ignored.append(sn)

# Terminate the fetch treads.
for i in range(max_connections):
    fetch_in_queue.put(None)

# We print a message if this takes too long.
time_to_print = time.time() + seconds_between_printing_thread_state
while threading.active_count() != t_active_count_after_fetch:
    # We keep waiting for this b/c this is what asks all the questions.
    # When this is done we know there is nothing else to put in downloads.
    shuttle_fetch_out_to_download_in()
    if time.time() > time_to_print:
        time_to_print = time.time() + seconds_between_printing_thread_state
        wrn("Waiting for fetch---bg thread count is %d,"
            " fetch_in_queue approx. length is %d." % (
            threading.active_count() - 1, fetch_in_queue.qsize()))

# Have to run it again b/c we don't know when the check/queueing was done.
shuttle_fetch_out_to_download_in()

for i in range(max_connections):
    download_in_queue.put(None)  # Terminate the download threads.


def collect_download_results():
    global download_out_queue
    global feeds_stats_downloads
    global feeds
    try:
        while True:
            sn, urls, msg = download_out_queue.get_nowait()
            if not sn in feeds_stats_downloads:
                feeds_stats_downloads[sn] = [0, 0, []]  # Success, skip, errors.
            if not urls:
                feeds_stats_downloads[sn][2].append(msg)
            else:
                feeds_stats_downloads[sn][0] = feeds_stats_downloads[sn][0] + 1
                for url in urls:
                    feeds[sn]["new_downloaded_links"].add(url)
    except:
        pass  # No items

# We print a message if this takes too long.
time_to_print = time.time() + seconds_between_printing_thread_state
while threading.active_count() != 1:  # Wait for all the download threads.
    collect_download_results()
    if time.time() > time_to_print:
        time_to_print = time.time() + seconds_between_printing_thread_state
        f_totsz = 0
        for f in os.listdir(work_dir):
            try:  # Files can be removed between listdir and getsize.
                f_totsz = f_totsz + os.path.getsize(os.path.join(work_dir, f))
            except:
                pass
        wrn("Waiting for downloads---bg thread count is %d,"
            " download_in_queue approx. size is %d, and approx. total size"
            " of files in the work dir is %s bytes." % (
                threading.active_count() - 1, download_in_queue.qsize(),
                '{:,}'.format(f_totsz), ))

# And then we run this again, in case anything got lost.
collect_download_results()

urllib.request.urlcleanup()

wrn("")

for sn in feeds_stats_ignored:
    wrn("Ignored %s" % sn)
for t in feeds_stats_unchanged:
    wrn("Unchanged %s (etag=%s, modified=%s)" % (t[0], t[1], t[2]))
for sn in feeds_stats_downloads:
    wrn("%s had %d successfull downloads, %d skips, and %d errors:" % (sn,
        feeds_stats_downloads[sn][0], feeds_stats_downloads[sn][1],
        len(feeds_stats_downloads[sn][2])))
    for err in feeds_stats_downloads[sn][2]:
        wrn("    %s" % err)
for t in feeds_stats_errors:
    wrn("Errors in %s: %s" % (t[0], t[1]))


wrn("")

wrn("%d total, %d ignored, %d unchanged, %d downloaded, and %d errors." % (
    len(feeds), len(feeds_stats_ignored), len(feeds_stats_unchanged),
    len(feeds_stats_downloads), len(feeds_stats_errors)))

if len(feeds) != \
        len(feeds_stats_ignored) + len(feeds_stats_unchanged) + \
        len(feeds_stats_downloads) + len(feeds_stats_errors):
    wrn("Total DOES NOT equal ignored + unchanged + downloaded + errors!!!!")
    wrn("What is happening?")

for sn in feeds:
    if feeds[sn]["new_downloaded_links"] and feeds[sn]["mode"] != "ignore":
        wrn("Saving etags, mod time, and d/l links for %s." % sn)
        with open(os.path.join(status_dir, sn), "w", encoding="UTF-8") as f:
            json.dump({
                "etag": feeds[sn]["etag"],
                "modified": feeds[sn]["modified"],
                "downloaded_links": list(feeds[sn]["new_downloaded_links"]),
            }, f)

if not len(os.listdir(work_dir)) == 0:
    wrn("Work dir \"%s\" is not empty, which means there are" % work_dir)
    wrn(" left-over files you may want to recover.  Delete them before")
    bye(" trying to re-run this script.")


wrn("Finished.")
